{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e0be80",
   "metadata": {},
   "source": [
    "![](SARSA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dac760",
   "metadata": {},
   "source": [
    "- Estado-Acci√≥n-Recompensa-Estado-Acci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ebde1",
   "metadata": {},
   "source": [
    "- Aprender la mejor acci√≥n en cada estado."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11596559",
   "metadata": {},
   "source": [
    "- SARSA toma en cuenta el esrado actual, como la acci√≥n actual para predecir la pr√≥xima acci√≥n y su recompensa.\n",
    "- Aprendizaje On policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796754ee",
   "metadata": {},
   "source": [
    "## ¬øQu√© significa esta f√≥rmula?\n",
    "\n",
    "Es la f√≥rmula de **actualizaci√≥n de Q en SARSA**. A diferencia de Q-learning, que usa el **mejor valor futuro posible**, SARSA actualiza el valor **basado en la acci√≥n realmente tomada** en el siguiente paso.\n",
    "\n",
    "## Descripci√≥n de cada t√©rmino:\n",
    "\n",
    "| S√≠mbolo     | Significado                                                                                                                       |\n",
    "| ----------- | --------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `Q(s, a)`   | Valor actual de tomar la acci√≥n `a` en el estado `s`.                                                                             |\n",
    "| `Œ±` (alfa)  | Tasa de aprendizaje (cu√°nto se ajusta el valor Q actual).                                                                         |\n",
    "| `r`         | Recompensa inmediata obtenida al hacer `a` desde `s`.                                                                             |\n",
    "| `Œ≥` (gamma) | Factor de descuento (cu√°nto importa el futuro).                                                                                   |\n",
    "| `Q(s', a')` | Valor esperado de tomar la **acci√≥n elegida** `a'` en el **nuevo estado** `s'`. **Aqu√≠ est√° la diferencia clave con Q-learning**. |\n",
    "\n",
    "---\n",
    "\n",
    "### ¬øQu√© hace SARSA?\n",
    "\n",
    "> Actualiza la tabla `Q` usando el valor de **la acci√≥n que realmente tom√≥ el agente**, no la mejor posible.\n",
    "\n",
    "Por eso su nombre:\n",
    "**S ‚Üí A ‚Üí R ‚Üí S' ‚Üí A'**\n",
    "(state, action, reward, next state, next action)\n",
    "\n",
    "## Comparaci√≥n con Q-learning\n",
    "\n",
    "| Algoritmo  | Usa en la actualizaci√≥n                   |\n",
    "| ---------- | ----------------------------------------- |\n",
    "| Q-learning | `max_a' Q(s', a')` (mejor acci√≥n posible) |\n",
    "| SARSA      | `Q(s', a')` (acci√≥n realmente tomada)     |\n",
    "\n",
    "### ¬øPor qu√© importa esto?\n",
    "\n",
    "* **SARSA es m√°s conservador**: aprende de lo que el agente *hizo*, no de lo que *hubiera sido mejor*.\n",
    "* Esto puede ser √∫til en **entornos donde el riesgo importa** (por ejemplo, evitar colisiones en rob√≥tica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5a78d740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "44da8a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensiones = (4, 4)\n",
    "estado_inicial = (0, 0)\n",
    "estado_final = (3, 3)\n",
    "acciones = [(0,-1), (0,1), (-1,0), (1,0)]\n",
    "acciones_simbolos = ['^', 'v', '<', '>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "079524af",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_estados = dimensiones[0] * dimensiones[1]\n",
    "num_acciones = len(acciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d91bf715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.zeros((num_estados, num_acciones))\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03424db4",
   "metadata": {},
   "source": [
    "- Alpha: \n",
    "  - Factor de la taza de aprendizaje. \n",
    "  - Cu√°nto se actualiza el valor Q en cada paso. \n",
    "  - Valor bajo es m√°s lento pero m√°s seguro\n",
    "- Gamma:\n",
    "  - Factor de descuento. \n",
    "  - Determinar la importancia de las recompensas que va a obtener en el futuro.\n",
    "  - Cercano a 1 hace que las recompensas futuras sean casi tan importantes como las inmediatas, haciendo que el agente considera consecuencias a largo plazo de sus acciones.\n",
    "  - Valos m√°s bajo, al agente va a valorar m√°s las consecuencias inmediatas.\n",
    "- Epsilon:\n",
    "  - Sirve para que el agente no repita siempre las mismas decisiones.\n",
    "  - Se define la probabilidad de que el agente tome una acci√≥n aleatoria en lugar de que el agente tome una acci√≥n conocida en la tabla Q.\n",
    "  - Permite que el agente explore el entorno.\n",
    "- Episodios:\n",
    "  - Define el n√∫mero total de veces que se va a repetir el proceso de entrenamiento.\n",
    "  - Empieza con el agente en el estado inicial y termina cuando alcance el objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3c884ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "episodios = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dee4b29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estado_a_indice(estado):\n",
    "    return estado[0] * dimensiones[1] + estado[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "60a1980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def elegir_accion(estado):\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return random.randint(0, num_acciones-1)\n",
    "    else:\n",
    "        return np.argmax(Q[estado_a_indice(estado)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bc8e5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_accion(estado, accion_idx):\n",
    "    \n",
    "    accion = acciones[accion_idx]\n",
    "    \n",
    "    nuevo_estado = tuple(np.add(estado, accion) % np.array(dimensiones))\n",
    "    \n",
    "    if nuevo_estado == estado_final:\n",
    "        recompensa = 1\n",
    "    else:\n",
    "        recompensa = -1\n",
    "    \n",
    "    return nuevo_estado, recompensa, nuevo_estado == estado_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "31e75545",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episodio in range(episodios):\n",
    "\n",
    "    estado = estado_inicial\n",
    "    terminado = False\n",
    "    accion_idx = elegir_accion(estado)\n",
    "\n",
    "    while not terminado:\n",
    "\n",
    "        nuevo_estado, recompensa, terminado = aplicar_accion(estado, accion_idx)\n",
    "\n",
    "        nuevo_accion_idx = elegir_accion(nuevo_estado)\n",
    "\n",
    "        indice = estado_a_indice(estado)\n",
    "\n",
    "        Q[indice, accion_idx] += alpha * (recompensa + gamma * Q[estado_a_indice(nuevo_estado), nuevo_accion_idx] - Q[indice, accion_idx])\n",
    "\n",
    "        estado, accion_idx = nuevo_estado, nuevo_accion_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cfd9ab22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['', '', '', ''],\n",
       "       ['', '', '', ''],\n",
       "       ['', '', '', ''],\n",
       "       ['', '', '', '']], dtype='<U2')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "politica_simbolos = np.empty(dimensiones, dtype=\"<U2\")\n",
    "politica_simbolos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5975e493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['^', 'v', '<', '<'],\n",
       "       ['>', '>', '<', '<'],\n",
       "       ['>', 'v', '>', '>'],\n",
       "       ['^', 'v', 'v', '^']], dtype='<U2')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(dimensiones[0]):\n",
    "    for j in range(dimensiones[1]):\n",
    "        estado = (i, j)\n",
    "        accion_idx = np.argmax(Q[estado_a_indice(estado)])\n",
    "        politica_simbolos[i, j] = acciones_simbolos[accion_idx]\n",
    "politica_simbolos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9e6b21ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Pol√≠tica aprendida con SARSA:\n",
      "\n",
      " ^   >   <   <  \n",
      " ^   ^   >   >  \n",
      " >   >   >   >  \n",
      " ^   v   v   üéØ \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definici√≥n del entorno: una cuadr√≠cula 4x4\n",
    "dimensiones = (4, 4)\n",
    "estado_inicial = (0, 0)\n",
    "estado_final = (3, 3)\n",
    "\n",
    "# Acciones posibles: izquierda, derecha, arriba, abajo\n",
    "acciones = [(0, -1), (0, 1), (-1, 0), (1, 0)]\n",
    "acciones_simbolos = ['^', 'v', '<', '>']\n",
    "\n",
    "# N√∫mero total de estados y acciones\n",
    "num_estados = dimensiones[0] * dimensiones[1]\n",
    "num_acciones = len(acciones)\n",
    "\n",
    "# Inicializaci√≥n de la tabla Q con ceros\n",
    "Q = np.zeros((num_estados, num_acciones))\n",
    "\n",
    "# Par√°metros de aprendizaje\n",
    "alpha = 0.1      # tasa de aprendizaje\n",
    "gamma = 0.99     # factor de descuento\n",
    "epsilon = 0.2    # probabilidad de explorar\n",
    "episodios = 1000 # n√∫mero total de episodios de entrenamiento\n",
    "\n",
    "# Convierte un estado (fila, columna) a un √≠ndice de la tabla Q\n",
    "def estado_a_indice(estado):\n",
    "    return estado[0] * dimensiones[1] + estado[1]\n",
    "\n",
    "# Elige una acci√≥n con pol√≠tica Œµ-greedy (exploraci√≥n/explotaci√≥n)\n",
    "def elegir_accion(estado):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.randint(0, num_acciones - 1)  # acci√≥n aleatoria\n",
    "    else:\n",
    "        return np.argmax(Q[estado_a_indice(estado)])  # mejor acci√≥n conocida\n",
    "\n",
    "# Aplica una acci√≥n al estado actual y devuelve:\n",
    "# nuevo estado, recompensa obtenida y si el episodio termin√≥\n",
    "def aplicar_accion(estado, accion_idx):\n",
    "    accion = acciones[accion_idx]\n",
    "    nuevo_estado = tuple(np.add(estado, accion) % np.array(dimensiones))  # wrap-around\n",
    "\n",
    "    # Recompensa +1 si se alcanza el estado objetivo, -1 en otro caso\n",
    "    if nuevo_estado == estado_final:\n",
    "        recompensa = 1\n",
    "    else:\n",
    "        recompensa = -1\n",
    "\n",
    "    return nuevo_estado, recompensa, nuevo_estado == estado_final\n",
    "\n",
    "# Entrenamiento usando SARSA\n",
    "for episodio in range(episodios):\n",
    "    estado = estado_inicial\n",
    "    terminado = False\n",
    "    accion_idx = elegir_accion(estado)  # elegir acci√≥n inicial\n",
    "\n",
    "    while not terminado:\n",
    "        # Aplicar la acci√≥n elegida\n",
    "        nuevo_estado, recompensa, terminado = aplicar_accion(estado, accion_idx)\n",
    "\n",
    "        # Elegir siguiente acci√≥n (SARSA aprende de la acci√≥n realmente tomada)\n",
    "        nuevo_accion_idx = elegir_accion(nuevo_estado)\n",
    "\n",
    "        # Obtener el √≠ndice del estado actual para acceder a la Q-table\n",
    "        indice = estado_a_indice(estado)\n",
    "\n",
    "        # Actualizar Q usando la f√≥rmula de SARSA\n",
    "        Q[indice, accion_idx] += alpha * (\n",
    "            recompensa + gamma * Q[estado_a_indice(nuevo_estado), nuevo_accion_idx]\n",
    "            - Q[indice, accion_idx]\n",
    "        )\n",
    "\n",
    "        # Avanzar al nuevo estado y acci√≥n\n",
    "        estado, accion_idx = nuevo_estado, nuevo_accion_idx\n",
    "\n",
    "\n",
    "def mostrar_politica_sarsa():\n",
    "    print(\"\\nPol√≠tica aprendida con SARSA:\\n\")\n",
    "    for fila in range(dimensiones[0]):\n",
    "        linea = \"\"\n",
    "        for col in range(dimensiones[1]):\n",
    "            estado = (fila, col)\n",
    "            if estado == estado_final:\n",
    "                linea += \" üéØ \"  # objetivo\n",
    "            else:\n",
    "                idx = estado_a_indice(estado)\n",
    "                mejor_accion = np.argmax(Q[idx])\n",
    "                linea += f\" {acciones_simbolos[mejor_accion]}  \"\n",
    "        print(linea)\n",
    "\n",
    "mostrar_politica_sarsa()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
