{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e048b7",
   "metadata": {},
   "source": [
    "![](Bellman.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73f6cff",
   "metadata": {},
   "source": [
    "### Ecuación de actualización en Q-learning:\n",
    "\n",
    "### ¿Qué significa?\n",
    "\n",
    "Esta ecuación **actualiza el valor de hacer una acción `a` en un estado `s`** con base en lo que el agente experimenta.\n",
    "Cada vez que el agente se mueve y observa una recompensa, mejora su \"tabla de decisiones\" llamada **Q-table**.\n",
    "\n",
    "### ¿Qué representa cada término?\n",
    "\n",
    "| Símbolo              | Significado                                                                                                                   |\n",
    "| -------------------- | ----------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `Q(s, a)`            | Valor actual estimado de hacer la acción `a` en el estado `s`.                                                                |\n",
    "| `α` (alfa)           | Tasa de aprendizaje: cuánto confías en lo nuevo que acabas de aprender (valor entre 0 y 1).                                   |\n",
    "| `R(s, a)`            | Recompensa inmediata que obtuviste por hacer `a` en `s`.                                                                      |\n",
    "| `γ` (gamma)          | Factor de descuento: cuánto te importa el futuro. Si es 0, solo importa el presente. Si es cercano a 1, te importa el futuro. |\n",
    "| `max_{a'} Q(s', a')` | Mejor valor posible desde el nuevo estado `s'`, mirando todas las acciones posibles `a'`.                                     |\n",
    "\n",
    "---\n",
    "\n",
    "### En palabras simples:\n",
    "\n",
    "> “Actualiza el valor de esta acción con lo que ya sabías, **más un pedacito de lo que aprendiste nuevo**: la recompensa obtenida y lo que esperas ganar en el futuro.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de03e5",
   "metadata": {},
   "source": [
    "- Permite a un agente imaginario aprender a tomar decisiones óptomas y a alcanzar un objetivo en ese entorno o ambiente.\n",
    "- Aprende con los valores que sean más convenientes en cada paso. A estos pasos los implementamos en un par de datos que definen la acción que tiene que tomar y el estado en el que queda."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6378049",
   "metadata": {},
   "source": [
    "- Agente que debe identificar que pasos realizar\n",
    "- Calidad de los pasos: Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb74801",
   "metadata": {},
   "source": [
    "- Valores Q ayudan al agente a tomar una decisión en cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c248999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d84bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición del entorno\n",
    "# ----------------------\n",
    "\n",
    "# Se establece una cuadrícula de 5x5 donde se desarrollará el proceso de aprendizaje.\n",
    "dimensiones = (5, 5)\n",
    "\n",
    "# El agente comienza en la celda (0, 0).\n",
    "estado_inicial = (0, 0)\n",
    "\n",
    "# El objetivo es llegar a la celda (4, 4).\n",
    "estado_objetivo = (4, 4)\n",
    "\n",
    "# Algunas celdas están bloqueadas por obstáculos y no son transitables.\n",
    "obstaculos = [(1, 1), (1, 3), (2, 3), (3, 0)]\n",
    "\n",
    "# Acciones disponibles: arriba, abajo, izquierda, derecha.\n",
    "# Cada acción es una tupla (Δfila, Δcolumna).\n",
    "acciones = [(-1, 0), (1, 0), (0, -1), (0, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4555b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Se calcula el número total de estados (celdas).\n",
    "num_estados = dimensiones[0] * dimensiones[1]\n",
    "num_estados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f386d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Y el número total de acciones posibles por estado.\n",
    "num_acciones = len(acciones)\n",
    "num_acciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ae722",
   "metadata": {},
   "source": [
    "### Inicialización de la tabla Q\n",
    "- Se crea una matriz de dimensiones (número de estados) x (número de acciones),\n",
    "- donde cada celda Q[s, a] representa el valor esperado de tomar la acción 'a' en el estado 's'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37df271",
   "metadata": {},
   "source": [
    "- Ejemplo:\n",
    "  - Q[12, 2] te diría: ¿Cuál es el valor esperado de estar en la celda que representa el estado 12 y moverse, por ejemplo, a la izquierda (acción 2)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10f215e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Crea una matriz Q de ceros, donde cada fila representa un estado y cada columna una acción.\n",
    "# Esta matriz guardará los valores Q, es decir, la utilidad esperada de tomar una acción en un estado.\n",
    "Q = np.zeros((num_estados, num_acciones))\n",
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb96ba1",
   "metadata": {},
   "source": [
    "- Toma una tupla estado = (fila, columna) y devuelve un entero que representa el número del estado en la Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08059e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de mapeo: estado a índice de la Q-table\n",
    "# ------------------------------------------------\n",
    "# Convierte un estado (fila, columna) en un índice entero correspondiente\n",
    "# a la fila de la tabla Q. Esto permite representar la cuadrícula 2D como una lista 1D.\n",
    "\n",
    "def estado_a_indice(estado):\n",
    "    return estado[0] * dimensiones[1] + estado[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c95ac32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "5\n",
      "1\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "ejemplo = estado_a_indice((0, 0))\n",
    "print(ejemplo)\n",
    "\n",
    "ejemplo = estado_a_indice((1, 0))\n",
    "print(ejemplo)\n",
    "\n",
    "ejemplo = estado_a_indice((0, 1))\n",
    "print(ejemplo)\n",
    "\n",
    "ejemplo = estado_a_indice((3, 3))\n",
    "print(ejemplo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd21580a",
   "metadata": {},
   "source": [
    "- Alpha: \n",
    "  - Factor de la taza de aprendizaje. \n",
    "  - Cuánto se actualiza el valor Q en cada paso. \n",
    "  - Valor bajo es más lento pero más seguro\n",
    "- Gamma:\n",
    "  - Factor de descuento. \n",
    "  - Determinar la importancia de las recompensas que va a obtener en el futuro.\n",
    "  - Cercano a 1 hace que las recompensas futuras sean casi tan importantes como las inmediatas, haciendo que el agente considera consecuencias a largo plazo de sus acciones.\n",
    "  - Valos más bajo, al agente va a valorar más las consecuencias inmediatas.\n",
    "- Epsilon:\n",
    "  - Sirve para que el agente no repita siempre las mismas decisiones.\n",
    "  - Se define la probabilidad de que el agente tome una acción aleatoria en lugar de que el agente tome una acción conocida en la tabla Q.\n",
    "  - Permite que el agente explore el entorno.\n",
    "- Episodios:\n",
    "  - Define el número total de veces que se va a repetir el proceso de entrenamiento.\n",
    "  - Empieza con el agente en el estado inicial y termina cuando alcance el objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d188b3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "gamma = 0.99\n",
    "epsilon = 0.2\n",
    "episodios = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393b5599",
   "metadata": {},
   "source": [
    "- Esta función aplica la estrategia ε-greedy, una técnica muy común en Q-learning para balancear:\n",
    "  - Exploración (probar nuevas acciones para descubrir recompensas) \n",
    "  - y Explotación (usar el conocimiento actual para tomar la mejor decisión)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "990b52b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para elegir una acción con estrategia ε-greedy\n",
    "# ------------------------------------------------------\n",
    "# Esta función implementa el balance entre exploración y explotación.\n",
    "# Con probabilidad 'epsilon', se elige una acción aleatoria (exploración).\n",
    "# Con probabilidad '1 - epsilon', se elige la mejor acción conocida\n",
    "# (la de mayor valor Q en la tabla para ese estado).\n",
    "# La función devuelve el índice de la acción elegida (0, 1, 2 o 3), según si el agente explora o explota.\n",
    "\n",
    "def elegir_accion(estado):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        return random.choice(range(0, num_acciones)) # Si explora (elige aleatoriamente)\n",
    "    else:\n",
    "        return np.argmax(Q[estado_a_indice(estado)]) # Si explota (elige la mejor conocida). Retorna el índice de la acción con mayor valor Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbaacab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aplicar_accion(estado, accion_idx):\n",
    "    \n",
    "    accion = acciones[accion_idx] # Se convierte el índice en una tupla que representa el movimiento.\n",
    "\n",
    "    # Calcula la nueva posición (fila, columna) del agente después de aplicar una acción.\n",
    "    # np.add(estado, accion): Suma elemento a elemento la posición actual y el movimiento. np.add((2, 2), (1, 0)) → array([3, 2])\n",
    "    # % dimensiones hace que si te pasas del límite de filas o columnas, vuelvas al otro lado del tablero. np.add((2, 2), (1, 0)) % (5, 5) → array([3, 2])  (sigue dentro)\n",
    "    # Convierte el resultado final de np.array a una tupla de Python\n",
    "    nuevo_estado = tuple(np.add(estado, accion) % dimensiones)\n",
    "\n",
    "    # Si el nuevo estado es un obstáculo o si el movimiento resultó en quedarse en el mismo lugar, entonces:\n",
    "    # No se mueve (estado se mantiene),\n",
    "    # Recibe una penalización fuerte: -100,\n",
    "    # El episodio no termina aún (False).\n",
    "    if nuevo_estado in obstaculos or nuevo_estado == estado:\n",
    "        return estado, -100, False\n",
    "\n",
    "    # Si el agente alcanza el estado objetivo:\n",
    "    # Se actualiza el estado correctamente,\n",
    "    # Recibe una gran recompensa de +100,\n",
    "    # Y se termina el episodio (True).\n",
    "    if nuevo_estado == estado_objetivo:\n",
    "        return nuevo_estado, 100, True\n",
    "    \n",
    "    # Caso general\n",
    "    # Movimiento válido, pero no llegó al objetivo.\n",
    "    # Penalización leve -1 para motivar que no se quede dando vueltas innecesarias.\n",
    "    # El episodio continúa (False).\n",
    "    return nuevo_estado, -1, False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f19dc",
   "metadata": {},
   "source": [
    "- Esta parte es el motor del algoritmo, donde el agente explora, se equivoca, acierta y va mejorando la tabla Q."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6eeb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for episodio in range(episodios): # Repetir por cada episodio (ciclo de entrenamiento)\n",
    "\n",
    "    estado = estado_inicial\n",
    "    terminado = False\n",
    "\n",
    "    while not terminado:\n",
    "        \n",
    "        # Convierte la posición (fila, columna) al índice numérico que corresponde a la fila en Q.\n",
    "        idx_estado = estado_a_indice(estado) \n",
    "\n",
    "        # Decide si explorar o explotar (ε-greedy), y elige una acción (índice 0 a 3).\n",
    "        accion_idx = elegir_accion(estado)\n",
    "\n",
    "        # Aplicar acción y obtener resultados\n",
    "        nuevo_estado, recompensa, terminado = aplicar_accion(estado, accion_idx)\n",
    "\n",
    "        # Calcular índice del nuevo estado. Necesario para actualizar Q con base en lo que viene.\n",
    "        idx_nuevo_estado = estado_a_indice(nuevo_estado)\n",
    "\n",
    "        # Actualizar la tabla Q. Se está aprendiendo cuánto vale tomar esa acción desde ese estado.\n",
    "        Q[idx_estado, accion_idx] = Q[idx_estado, accion_idx] + alpha * (recompensa + gamma * np.max(Q[idx_nuevo_estado]) - Q[idx_estado, accion_idx])\n",
    "        \n",
    "        #  Moverse al nuevo estado\n",
    "        estado = nuevo_estado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79d542f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabla Q (valores por estado y acción):\n",
      "Estado (0, 0) (0): [97.93321383 -0.15760053 18.60181009  9.70406378]\n",
      "Estado (0, 1) (1): [  0.66303064 -18.30815603  42.73183634   0.        ]\n",
      "Estado (0, 2) (2): [-0.1 -0.1  0.   0. ]\n",
      "Estado (0, 3) (3): [0. 0. 0. 0.]\n",
      "Estado (0, 4) (4): [71.75704635  1.55984524  0.          0.        ]\n",
      "Estado (1, 0) (5): [ -0.1         -0.19         3.75857798 -10.        ]\n",
      "Estado (1, 1) (6): [0. 0. 0. 0.]\n",
      "Estado (1, 2) (7): [-0.1 -0.1  0.   0. ]\n",
      "Estado (1, 3) (8): [0. 0. 0. 0.]\n",
      "Estado (1, 4) (9): [24.5296263   0.         -7.95784019  0.        ]\n",
      "Estado (2, 0) (10): [ -0.16072032 -19.          -0.1         -0.1099    ]\n",
      "Estado (2, 1) (11): [-10.      -0.1     -0.1999  -0.1   ]\n",
      "Estado (2, 2) (12): [-0.1 -0.1 -0.1  0. ]\n",
      "Estado (2, 3) (13): [0. 0. 0. 0.]\n",
      "Estado (2, 4) (14): [-0.1  0.   0.   0. ]\n",
      "Estado (3, 0) (15): [0. 0. 0. 0.]\n",
      "Estado (3, 1) (16): [ -0.19  -0.1  -10.    -0.1 ]\n",
      "Estado (3, 2) (17): [-0.1    -0.1    -0.1099  0.    ]\n",
      "Estado (3, 3) (18): [0. 0. 0. 0.]\n",
      "Estado (3, 4) (19): [0. 0. 0. 0.]\n",
      "Estado (4, 0) (20): [-15.64590125  51.96688216  99.99059539   9.80204691]\n",
      "Estado (4, 1) (21): [-0.19       -0.19       45.64216119 -0.1       ]\n",
      "Estado (4, 2) (22): [-0.1    -0.1    -0.1099  0.    ]\n",
      "Estado (4, 3) (23): [0. 0. 0. 0.]\n",
      "Estado (4, 4) (24): [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def mostrar_q_table():\n",
    "    print(\"Tabla Q (valores por estado y acción):\")\n",
    "    for fila in range(dimensiones[0]):\n",
    "        for col in range(dimensiones[1]):\n",
    "            estado = (fila, col)\n",
    "            idx = estado_a_indice(estado)\n",
    "            print(f\"Estado {estado} ({idx}): {Q[idx]}\")\n",
    "\n",
    "mostrar_q_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a215e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Política aprendida (mejor acción por estado):\n",
      "\n",
      " ↑   ←   ←   ↑   ↑  \n",
      " ←   ⛔  ←   ⛔  ↑  \n",
      " ←   ↓   →   ⛔  ↓  \n",
      " ⛔  ↓   →   ↑   ↑  \n",
      " ←   ←   →   ↑   🎯 \n"
     ]
    }
   ],
   "source": [
    "def mostrar_politica():\n",
    "    simbolos_accion = {\n",
    "        0: '↑',   # (-1, 0)\n",
    "        1: '↓',   # (1, 0)\n",
    "        2: '←',   # (0, -1)\n",
    "        3: '→'    # (0, 1)\n",
    "    }\n",
    "\n",
    "    print(\"\\nPolítica aprendida (mejor acción por estado):\\n\")\n",
    "    for fila in range(dimensiones[0]):\n",
    "        linea = \"\"\n",
    "        for col in range(dimensiones[1]):\n",
    "            estado = (fila, col)\n",
    "            if estado in obstaculos:\n",
    "                linea += \" ⛔ \"  # obstáculo\n",
    "            elif estado == estado_objetivo:\n",
    "                linea += \" 🎯 \"  # objetivo\n",
    "            else:\n",
    "                idx = estado_a_indice(estado)\n",
    "                mejor_accion = np.argmax(Q[idx])\n",
    "                linea += f\" {simbolos_accion[mejor_accion]}  \"\n",
    "        print(linea)\n",
    "\n",
    "mostrar_politica()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb317c1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politica aprendida (0:arriba, 1:abajo, 2:izquierda, 3:derecha):\n",
      "[[0 2 2 0 0]\n",
      " [2 0 2 0 0]\n",
      " [2 1 3 0 1]\n",
      " [0 1 3 0 0]\n",
      " [2 2 3 0 0]]\n"
     ]
    }
   ],
   "source": [
    "politica = np.zeros(dimensiones, dtype=int)\n",
    "\n",
    "for i in range(dimensiones[0]):\n",
    "    for j in range(dimensiones[1]):\n",
    "        estado = (i, j)\n",
    "        idx_estado = estado_a_indice(estado)\n",
    "        mejor_accion = np.argmax(Q[idx_estado])\n",
    "        \n",
    "        politica[i, j] = mejor_accion\n",
    "\n",
    "print(\"Politica aprendida (0:arriba, 1:abajo, 2:izquierda, 3:derecha):\")\n",
    "print(politica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ec0eb1",
   "metadata": {},
   "source": [
    "### Ejercicios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61f07a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Parámetros globales para la función elegir_accion\n",
    "epsilon = 0.1\n",
    "num_acciones = 4\n",
    "Q = np.zeros((25, num_acciones))  # Ejemplo de matriz Q\n",
    "dimensiones = (5, 5)\n",
    "\n",
    "def estado_a_indice(estado):\n",
    "    # Convierte el estado en un índice para la matriz Q\n",
    "    return estado[0] * dimensiones[1] + estado[1]\n",
    "\n",
    "def elegir_accion(estado):\n",
    "    # Implementa la estrategia epsilon-greedy\n",
    "    if random.uniform(0,1) < epsilon:\n",
    "        return random.choice(range(0, num_acciones))\n",
    "    else:\n",
    "        return np.argmax(Q[estado_a_indice(estado)])\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
